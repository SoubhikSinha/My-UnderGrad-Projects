REVIEW 1 : EXTRAS
=================

- RMSE (Root Mean Squared Error) : 
It is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.

- R-Square (R2) score : 
â€œ(total variance explained by model) / total variance.â€ So if it is 100%, the two variables are perfectly correlated, i.e., with no variance at all. A low value would show a low level of correlation, meaning a regression model that is not valid, but not in all cases. Here, the two variables can be assumed to be the testing data values and predicted data values.

NOTE ðŸ’¥ : Higher R2 score and lower RMSE score --> Better Model

===========================================================================================
ðŸŸ©Paper [1] :
------------

- SVR (Support Vector Regression) : Support Vector Regression is a supervised learning algorithm that is used to predict discrete values. Support Vector Regression uses the same principle as the SVMs. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points.

- PSO (Particle Swarm Optimization) : The process of finding optimal values for the specific parameters of a given system to fulfill all 
design requirements while considering the lowest possible cost is referred to as an optimization. PSO uses a number of particles (agents) that constitute a swarm moving around in the search space, looking for the best solution. Each particle in the swarm looks for its positional coordinates in the solution space, which are associated with the best solution that has been achieved so far by that particle. It is known as pbest or personal best.

- ARIMA : An autoregressive integrated moving average, or ARIMA, is a statistical analysis model that uses time series data to either better understand the data set or to predict future trends.

- ANN (Artificial Neural Network) : Usually called neural networks (NNs) or neural nets, they are computing systems inspired by the biological neural networks that constitute animal brains (Neurons).

- ACF (Auto-Correlation Function) : 
The autocorrelation function is a statistical representation used to analyze the degree of similarity between a time series and a lagged 
version of itself. This function allows the analyst to compare the current value of a data set to its past value. To use this function, the analyst uses the same time series and compares it against a lagged version of itself over one or more time periods. They assess the strength of the correlation between these different versions to uncover trends and patterns that allow them to evaluate the strength of the relationship between two or more variables.

- PACF (Partial Auto-Correlation Function) :
In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a stationary time series with its own
lagged values, regressed the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not 
control for other lags.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [2] :
------------

- Adaptive Fourier Decomposition (AFD) : 
The Fourier decomposition is the classical tool for decomposing a given surface or more generally a signal into its basic components. Using Fourier decomposition the surface is broken down into different scales using a set of sine and cosine functions with different frequencies. AFD uses adaptive basis to achieve the fast energy convergence.

- Metaheuristics are strategies that guide the search process. The goal is to efficiently explore the search space in order to find nearâ€“optimal solutions.

- Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa.

- Sine Cosine Optimization : The algorithm generates various initial random solutions and asks them to shift towards the best solution using a mathematical model based on sine and cosine functions.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [3] :
------------

- Holt-Winters exponential smoothing : 
The Holt-Winters method uses exponential smoothing to encode lots of values from the past and use them to predict â€œtypicalâ€ values for the 
present and future. Exponential smoothing refers to the use of an exponentially weighted moving average (EWMA) to â€œsmoothâ€ a time series.

- Fruit fly optimization process : 
The fruit fly optimization algorithm (FOA) was a meta-heuristic algorithm which is inspired by the foraging behavior of fruit fly. Fruit fly relies on vision and smell to position food during foraging. FOA searches for solution space by mimicking the way of fruit fly flight when solving optimization problems.

- The genetic algorithm (GA) is a method for solving both constrained and unconstrained optimization problems that is based on natural selection, the process that drives biological evolution. Genetic Algorithms (GAs) are used to optimize free parameters of SVR. The experimental results indicate that GA-SVR can achieve better forecasting accuracy and performance than traditional SVR and artificial neural network (ANN) prediction models.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [4] :
------------

- Temporal Aggregation (MTA) : The situation in which a variable that evolves through time can not be observed at all dates.

- Multiple Aggregation Prediction Algorithm (MAPA) : MAPA can be seen as a three step procedure, where in the first step the original time series is aggregated in multiple aggregation levels using non-overlapping means of length k. The mean is used instead of the sum, as it retains the scale of the series across the various aggregation levels. 

- Hierarchical forecasting :  It is the process of generating coherent forecasts (or reconciling incoherent forecasts) that allows individual time series to be forecasted individually while still preserving the relationships within the hierarchy.

- Cross-sectional refers to hierarchies that include different time series of the same sampling frequency across various demarcations, such as product categories, geographical regions, variable components, etc.

- (Flaw) MTA & MAPA : the exact demand from a specific region can't be ascertained, because it is aggregated.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [5] :
------------

- Data granularity is a measure of the level of detail in a data structure. In time-series data, for example, the granularity of measurement might be based on intervals of years, months, weeks, days, or hours.

- Boosted Tree : Boosting means that each tree is dependent on prior trees. The algorithm learns by fitting the residual of the trees that preceded it. Thus, boosting in a decision tree ensemble tends to improve accuracy with some small risk of less coverage.

- SVM : SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.

- The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression.

- Random Forest : Random forest is a commonly-used machine learning algorithm, which combines the output of multiple decision trees to reach a single result.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [6] :
------------

- Rough Set Theory : Rough set theory has been a methodology of database mining or knowledge discovery in relational databases. In its abstract form, it is a new area of uncertainty mathematics closely related to fuzzy theory. We can use rough set approach to discover structural relationship within imprecise and noisy data.

- Deep Neural Network : A deep neural network (DNN) is an ANN with multiple hidden layers between the input and output layers. Similar to shallow ANNs, DNNs can model complex non-linear relationships.

- Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and feeding this loss backward through the neural network layers to fine-tune the weights. Backpropagation is the essence of neural net training.

- Elman neural network is a kind of feedback neural network; based on BP neural network hidden layer adds an undertake layer, as the delay operator, the purpose of memory, so that the network system has ability to adapt to the time-varying dynamic characteristics and has strong global stability.

- A fuzzy neural network or neuro-fuzzy system is a learning machine that finds the parameters of a fuzzy system (i.e., fuzzy sets, fuzzy rules) by exploiting approximation techniques from neural networks.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [7] :
------------

- K-Nearest Neighbour (K-NN) : a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.

- RMSE (Root Mean Squared Error) : the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are.

- NRMSE (Normalized RMSE) : Normalizing the RMSE facilitates the comparison between datasets or models with different scales. 

- MAPE (Mean Absolute Percentage Error) : It measures the accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [8] :
------------

- Dynamic Regression : Similar to Regression Analysis, but it is believed to produce more realistic results because it emphasizes the ripple effects, the input variables can have on the dependent variable.

- Prophet : a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.

- Seasonal Autoregressive Integrated Moving Average (SARIMA) : an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.

- âš¡âš¡Bootstrapping is a term used in business to refer to the process of using only existing resources, such as personal savings, personal computing equipment, and garage space, to start and grow a company.

-------------------------------------------------------------------------------------------
ðŸŸ©Paper [9] : 
------------

- Ensemble-based machine learning methods : techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. The combined models increase the accuracy of the results significantly. This has boosted the popularity of ensemble methods in machine learning.

- âš¡âš¡Quantile : Each of any set of values of a variate which divide a frequency distribution into equal groups, each containing the same fraction of the total population.

- Quantile Random Forest : Quantile regression forests (QRF) is an extension of random forests that provides non-parametric estimates of the median predicted value as well as prediction quantiles. It therefore allows spatially explicit non-parametric estimates of model uncertainty.

- Bagging and Boosting Methods : Bagging is used when our objective is to reduce the variance of a decision tree. Here the concept is to create a few subsets of data from the training sample, which is chosen randomly with replacement. Now each collection of subset data is used to prepare their decision trees thus, we end up with an ensemble of various models. The average of all the assumptions from numerous tress is used, which is more powerful than a single decision tree. Boosting is another ensemble procedure to make a collection of predictors. In other words, we fit consecutive trees, usually random samples, and at each step, the objective is to solve net error from the prior trees.

-----------------------------------------------------------------------
ðŸŸ©Paper [10] : 
-------------

- ANN : ðŸ”²EXPLAIN wrt earlier definition

- ARIMA EXTRAS
  >> Persistence Forecast : A forecast that the current weather condition will persist and that future weather will be the same as the present.

///////////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////

REVIEW 2 : EXTRAS
=================

âš¡PROPOSED METHODOLOGY ðŸ”½
--------------------------
- An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights.

- 

- Why using MinMaxScaler ?
1. When the upper and lower boundaries are well known from domain knowledge.
2. Scales all the data features in the range [0, 1] or else in the range [-1, 1] if there are negative values in the dataset.
3. Guarantees all features will have the exact same scale

- Keras
Keras is an open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.Keras is a high-level, deep learning API developed by Google for implementing neural networks. It is written in Python and is used to make the implementation of neural networks easy. It also supports multiple backend neural network computation.

- Keras vs Tensorflow
TensorFlow is an open-sourced end-to-end platform, a library for multiple machine learning tasks, while Keras is a high-level neural network library that runs on top of TensorFlow. Both provide high-level APIs used for easily building and training models, but Keras is more user-friendly because it's built-in Python.

- 3-Dimensional data
1. The batch size is a number of samples processed before the model is updated. The number of epochs is the number of complete passes through the training dataset. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.
2. A time step is a single occurrence of the cell
3. input dimension / input shape : the term says it all

- Rolling Statistics
A rolling average continuously updates the average of a data set to include all the data in the set until that point.

- Augmented Dickey-Fuller (ADF) Test
Augmented Dickey Fuller test (ADF Test) is a common statistical test used to test whether a given Time series is stationary or not. It is one of the most commonly used statistical test when it comes to analyzing the stationary of a series.


>> De-trending data ðŸ”»(Below 4 only)

âž¡ï¸ BoxCox Transformation
Box-Cox transformation is a statistical technique that transforms your target variable so that your data closely resembles a normal distribution.

âž¡ï¸ Logarithmic Transformation
The log transformation is often used where the data has a positively skewed distribution (shown below) and there are a few very large values. If these large values are located in your study area, the log transformation will help make the variances more constant and normalize your data.

âž¡ï¸ Removing trend using Moving Average
A moving average smoothes a series by consolidating the a days' data-points into longer units of timeâ€”namely an average of several days' data.

âž¡ï¸ Exponential (Decay) transformation
An exponential transformation is a simple algebraic transformation of a monomial function through variable substitution with an exponential variable.



- ACF (Auto-Correlation Function)
It defines how data points in a time series are related, on average, to the preceding data points. In other words, it measures the self-similarity of the signal over different delay times.

- PACF (Partial ACF)
It gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags.

â˜ ï¸NOTE : The â€œIâ€ stands for integrated in ARIMA, which means that the data is stationary.

- Regression equation : It is used in stats to find out what relationship, if any, exists between sets of data.


NOTE : For all other components uncovered here - are properly explained in the doc

âš¡IMPLEMENTATION ðŸ”½
--------------------
âœ¨For both LSTM & RNN --

- The " %matplotlib inline " command tells the IPython environment to draw the plots immediately after the current cell.

- The info() method prints information about the DataFrame. The information contains the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values).



(RNN) KERAS ðŸ”»ðŸ”»ðŸ”»

> Sequential : A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.

> Tensor : A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor.

> Dense : Keras Dense layer is the layer that contains all the neurons that are deeply connected within themselves. This means that every neuron in the dense layer takes the input from all the other neurons of the previous layer. We can add as many dense layers as required. It is one of the most commonly used layers.

> SimpleRNN : The complete RNN layer is presented as SimpleRNN class in Keras. Each RNN cell takes one data input and one hidden state which is passed from a one-time step to the next.

-------------------------------------------------------------------------------------------

> Dropout : Dropout is a technique where randomly selected neurons are ignored during training. They are â€œdropped outâ€ randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.
|
|=> CODE : ðŸ–¥ï¸ model_rnn.add(Dropout(0.2))

âž¡ï¸ Randomly selecting nodes to be dropped out with a given probability (e.g., 20% or 0.2) in each weight update cycle

âœ¨ DROPOUT Layer is created to reduce the overfitting of neural networks.

-------------------------------------------------------------------------------------------

ðŸ–¥ï¸ model_rnn.add(SimpleRNN(units = 50,activation = "tanh", return_sequences = True , input_shape = (x_train.shape[1],3))) ðŸ”»

	> Units : The output dimesion of model. Units will be the shape of the models' 	internal state.

	> Return Sequences : Refer to return the hidden state

ðŸ–¥ï¸ model_rnn.add(Dense(units = 1)) ==> Dense layer feeds all outputs from the previous layer to all its neurons. "Units" -> Positive integer, dimensionality of the output space.


ðŸ–¥ï¸ model_rnn.compile(optimizer = "adam", loss = "mean_squared_error") ðŸ”»

	> optimizer : assists in improving the accuracy and reduces the total loss.

	> Adam is an alternative optimization algorithm that provides more efficient neural 	network weights by running repeated cycles of â€œadaptive moment estimation.â€



(LSTM) KERAS ðŸ”»ðŸ”»ðŸ”»

> Sequential : A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.

> Dense : Keras Dense layer is the layer that contains all the neurons that are deeply connected within themselves. This means that every neuron in the dense layer takes the input from all the other neurons of the previous layer. We can add as many dense layers as required. It is one of the most commonly used layers.

> Units : The output dimesion of model. Units will be the shape of the models' internal state.

> Return Sequences : Refer to return the hidden state



---------
| ARIMA |ðŸ”»ðŸ”»ðŸ”»
---------

- statsmodel -> seasonal decompose :  lets you decompose a time series into trend, seasonality and noise in one line of code.

- statsmodel -> adfuller : ADF (Augmented Dickey-Fuller) Test

- rcparams : Each time Matplotlib loads, it defines a runtime configuration (rc) containing the default styles for every plot element you create.

- ðŸ–¥ï¸ rolling_mean = data.rolling(window=12).mean()
  ðŸ–¥ï¸ rolling_std = data.rolling(window=12).std()
  
Here, the moving average will be calculated by taking those many samples at a time, denoted as 'window=12'.

- In ADF Test, the critical values are already set (as mentioned in the output)

- In ADF Test => t statistic = test statistic (value/outcome of the test)

- In ADF Test => The p value is a number, calculated from a statistical test, that describes how likely you are to have found a particular set of observations if the null hypothesis were true.

- ðŸ–¥ï¸ adf = adfuller(ts, autolag='AIC') ðŸ”»

  > Autolag : Method to use when automatically determining the lag length among the values 0, 1, â€¦, maxlag (AIC -> by-default)

- BoxCox Transformation : a statistical technique that transforms your target variable so that your data closely resembles a normal distribution

- Exponential (Decay) Transformation : a process in which a quantity decreases over time, with the rate of decrease becoming proportionally smaller as the quantity gets smaller.

- In PACF => method = 'ols' : regression of time series on lags of it and on constant.

- Persistent model : A forecast that the current value conditions will persist and that future values will be the same as the present.

- Auto-Regressive Model : forecasts future behavior based on past behavior data.

- In ARIMA :
	
	> p is the number of autoregressive terms,
	> d is the number of nonseasonal differences,
	> q is the number of lagged forecast errors in the prediction equation.